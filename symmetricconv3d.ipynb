{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, math\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter \nfrom torch import Tensor\nfrom torch.nn import functional as F\nfrom typing import Optional, List, Tuple, Union\nclass SymmetricConv3d(nn.Conv3d):\n    def __init__(\n            self,\n            in_channels: int,\n            out_channels: int,\n            kernel_size,\n            stride = 1,\n            padding = 0,\n            dilation = 1,\n            groups: int = 1,\n            bias: bool = False,\n            padding_mode: str = 'zeros', \n            symmetry: dict = {},\n            share_bias: bool = False\n    ):     \n        '''\n        Args:\n        symmetry (dict) - number of filters that are symmetric about the horizontal, \n                          vertical, z axis, or any combination of them\n                          e.g. {'h':4, 'z': 2, 'hv':8, 'hvz':8} has 4 filters (2 filter pairs) that are \n                          horizontally symmetric, 2 filters (1 filter pair) which are symmetric \n                          about the z axis, 8 filters (2 filter quadruples) that are symmetric \n                          horizontally and vertically, and 8 filters (1 set of 8 filters) that are\n                          symmetric about all three axes\n        share_bias (bool) - if True, symmetric filter pairs also share their biases\n        '''   \n        super(SymmetricConv3d, self).__init__(\n            in_channels, out_channels, kernel_size, stride, padding, dilation,\n            groups, bias, padding_mode)\n        if self.groups > 1: raise ValueError(self.__str__() + ' does not support groups>1')\n        if not bias: \n            self.share_bias = False\n        else:\n            self.share_bias = share_bias\n        if symmetry is None: \n            # no symmetry, return a standard Conv2d\n            self.symmetry = None\n        else:\n            # Set defaults for symmetric filters pairs\n            symmetry = dict(symmetry) # make a copy\n            symmetry.setdefault('h', 0)\n            symmetry.setdefault('v', 0)\n            symmetry.setdefault('z', 0)\n            symmetry.setdefault('hv', 0)\n            symmetry.setdefault('hz', 0)\n            symmetry.setdefault('vz', 0)\n            symmetry.setdefault('hvz', 0)\n            self.symmetry = symmetry\n\n            # sanity check: number of filters divisible by 2 resp. 4?\n            for key, val in symmetry.items():\n                    if key not in ('h','v','z','hv','hz','vz','hvz'):\n                        raise ValueError(\"Unknown key, use only 'h','v','z','hv','hz','vz','hvz'\")\n                    elif (key in ('h','v','z')) and (val % 2 != 0):\n                        raise ValueError('Number of symmetric h and v filters must be divisible by 2')\n                    elif (key in ('hv','hz','vz')) and (val % 4 != 0):\n                        raise ValueError('Number of symmetric hv filters must be divisible by 4')\n                    elif (key=='hvz') and (val % 8 != 0):\n                        raise ValueError('Number of symmetric hv filters must be divisible by 8')\n            # sanity check: number of symmetric filters must be <= number of filters\n            print(sum(list(symmetry.values())))\n            assert sum(list(symmetry.values())) <= self.out_channels, \"Number of symmetric channels exceeds number of out channels\"\n            self.unique_out_channels = self.out_channels - symmetry['h']//2 - symmetry['v']//2 - symmetry['z']//2 \\\n                - 3*symmetry['hv']//4 - 3*symmetry['hz']//4 - 3*symmetry['vz']//4 \\\n                - 7*symmetry['hvz']//8\n\n\n            # Create only the unique weights \n            if self.transposed:\n                self.weight = Parameter(torch.Tensor(\n                    in_channels, self.unique_out_channels, *self.kernel_size))\n            else:\n                self.weight = Parameter(torch.Tensor(\n                    self.unique_out_channels, in_channels, *self.kernel_size))\n\n        self.reset_parameters()\n    \n    def _conv_forward(self, input: Tensor, weight: Tensor, bias=None):\n        if self.padding_mode != \"zeros\":\n            return F.conv3d(\n                F.pad(\n                    input, self._reversed_padding_repeated_twice, mode=self.padding_mode\n                ),\n                weight,\n                bias,\n                self.stride,\n                _triple(0),\n                self.dilation,\n                self.groups,\n            )\n        return F.conv3d(\n            input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n        )\n    \n    def forward(self, input):\n        '''\n        Starting from the unique weights, use torch.flip calls to create their \n        symmetric counterparts. Then concatenate all kernels and forward the resultant weights.\n        '''\n        s = self.symmetry\n        weight = [self.weight]\n        ix = 0\n        if s['h'] > 0:\n            weight.append(torch.flip(self.weight[ix:ix+s['h']//2,:,:,:], (4,)))\n            ix += s['h']//2\n        if s['v'] > 0:\n            weight.append(torch.flip(self.weight[ix:ix+s['v']//2,:,:,:], (3,)))\n            ix += s['v']//2\n        if s['z'] > 0:\n            weight.append(torch.flip(self.weight[ix:ix+s['z']//2,:,:,:], (2,)))\n            ix += s['z']//2\n        if s['hv'] > 0:\n            n = s['hv']//4\n            weight.extend([torch.flip(self.weight[ix:ix + n,:,:,:], (4,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (3,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (3,4))])\n            ix += n\n        if s['hz'] > 0:\n            n = s['hz']//4\n            weight.extend([torch.flip(self.weight[ix:ix + n,:,:,:], (4,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,4))])\n            ix += n\n        if s['vz'] > 0:\n            n = s['vz']//4\n            weight.extend([torch.flip(self.weight[ix:ix + n,:,:,:], (3,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,3))])\n            ix += n\n        if s['hvz'] > 0:\n            n = s['hvz']//8\n            weight.extend([\n            torch.flip(self.weight[ix:ix + n,:,:,:], (4,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (3,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,3)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,4)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (3,4)),\n            torch.flip(self.weight[ix:ix + n,:,:,:], (2,3,4))\n            ])\n            ix += n\n        return self._conv_forward(input=input, weight=torch.cat(weight, dim=0))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}