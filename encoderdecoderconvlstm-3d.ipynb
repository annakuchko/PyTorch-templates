{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nfrom collections import OrderedDict\n\nclass ConvLSTMCell(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n        \"\"\"\n        Initialize ConvLSTM cell.\n        Parameters\n        ----------\n        input_dim: int\n            Number of channels of input tensor.\n        hidden_dim: int\n            Number of channels of hidden state.\n        kernel_size: (int, int)\n            Size of the convolutional kernel.\n        bias: bool\n            Whether or not to add the bias.\n        \"\"\"\n\n        super(ConvLSTMCell, self).__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.kernel_size = kernel_size\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n        self.bias = bias\n        self.conv = nn.Sequential(OrderedDict([\n#             ('dp0', nn.Dropout(0.35)),\n            ('conv0', nn.Conv2d(\n                in_channels=self.input_dim + self.hidden_dim,\n                out_channels=4 * self.hidden_dim,\n                kernel_size=self.kernel_size,\n                padding=self.padding,\n                bias=self.bias)),\n#             ('dp1', nn.Dropout(0.35)),\n#             ('conv1', nn.Conv2d(\n#                 in_channels=2 * self.hidden_dim,\n#                 out_channels=4 * self.hidden_dim,\n#                 kernel_size=self.kernel_size,\n#                 padding=self.padding,\n#                 bias=self.bias)),\n#             ('dp1', nn.Dropout(0.75)),\n#             ('inorm1', nn.InstanceNorm2d(128)),\n#             ('conv2', nn.Conv2d(\n#                 in_channels=8 * self.hidden_dim,\n#                 out_channels=4 * self.hidden_dim,\n#                 kernel_size=self.kernel_size,\n#                 padding=self.padding,\n#                 bias=self.bias)),\n#             ('dp2', nn.Dropout(0.25)),\n#             ('inorm2', nn.InstanceNorm2d(128)),\n#             ('conv3', nn.Conv2d(\n#                 in_channels=6 * self.hidden_dim,\n#                 out_channels=4 * self.hidden_dim,\n#                 kernel_size=self.kernel_size,\n#                 padding=self.padding,\n#                 bias=self.bias)),\n        ]))\n     \n    def forward(self, input_tensor, cur_state):\n        h_cur, c_cur = cur_state\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n        combined_conv = nn.functional.avg_pool2d(self.conv(combined),kernel_size=(3,3), padding=(1,1), stride=(1,1))\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n        \n        del combined_conv, cur_state, input_tensor, combined, h_cur\n        \n        i = torch.sigmoid(cc_i)\n        del cc_i\n        f = torch.sigmoid(cc_f)\n        del cc_f\n        o = torch.sigmoid(cc_o)\n        del cc_o\n        g = torch.tanh(cc_g)\n        del cc_g\n        \n        c_next = f * c_cur + i * g\n        del f, c_cur, i, g\n        h_next = o * torch.tanh(c_next)\n        del o\n\n        return h_next, c_next\n\n    def init_hidden(self, batch_size, image_size):\n        height, width = image_size\n        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.conv0.weight.device),    \n                    torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.conv0.weight.device))\n    \nclass EncoderDecoderConvLSTM(nn.Module):\n    def __init__(self, nf, in_chan, generator=False):\n        super(EncoderDecoderConvLSTM, self).__init__()\n\n        \"\"\" ARCHITECTURE \n        # Encoder (ConvLSTM)\n        # Encoder Vector (final hidden state of encoder)\n        # Decoder (ConvLSTM) - takes Encoder Vector as input\n        # Decoder (3D CNN) - produces regression predictions for our model\n        \"\"\"\n        self.generator = generator\n        self.encoder_1_convlstm = ConvLSTMCell(input_dim=in_chan,\n                                               hidden_dim=2*nf,\n                                               kernel_size=(3, 3),\n                                               bias=True,\n                                               )\n\n        self.encoder_2_convlstm = ConvLSTMCell(input_dim=2*nf,\n                                               hidden_dim=nf,\n                                               kernel_size=(3, 3),\n                                               bias=True,\n                                              )\n#         self.encoder_3_convlstm = ConvLSTMCell(input_dim=3*nf,\n#                                                hidden_dim=nf,\n#                                                kernel_size=(3, 3),\n#                                                bias=True,\n#                                               )\n#         self.encoder_4_convlstm = ConvLSTMCell(input_dim=2*nf,\n#                                                hidden_dim=nf,\n#                                                kernel_size=(3, 3),\n#                                                bias=True,\n#                                               )\n\n        self.decoder_1_convlstm = ConvLSTMCell(input_dim=nf,  \n                                               hidden_dim=2*nf,\n                                               kernel_size=(3, 3),\n                                               bias=True,\n                                              )\n\n        self.decoder_2_convlstm = ConvLSTMCell(input_dim=2*nf,\n                                               hidden_dim=nf,\n                                               kernel_size=(3, 3),\n                                               bias=True,\n                                              )\n#         self.decoder_3_convlstm = ConvLSTMCell(input_dim=2*nf,\n#                                                hidden_dim=nf,\n#                                                kernel_size=(3, 3),\n#                                                bias=True,\n#                                               )\n#         self.decoder_4_convlstm = ConvLSTMCell(input_dim=2*nf,\n#                                                hidden_dim=nf,\n#                                                kernel_size=(3, 3),\n#                                                bias=True,\n#                                               )\n\n        self.decoder_CNN = nn.Sequential(\n                nn.Dropout(0.95),\n                nn.BatchNorm3d(nf),\n#                 ConvLSTMCell(input_dim=nf,hidden_dim=nf, kernel_size=(3, 3),bias=True),\n#                 nn.Conv2d(in_channels=nf,\n#                           out_channels=1,\n#                           kernel_size=( 4, 4),\n#                           padding=(1, 1),\n#                           stride=(2,2),\n#                           bias=True)\n            \n            )\n        \n            \n    def autoencoder(self, x, seq_len, future_step, \n                    h_t, c_t, \n                    h_t2, c_t2, \n#                     h_t3, c_t3, \n#                     h_t4, c_t4, \n                    h_t5, c_t5, \n                    h_t6, c_t6,\n#                     h_t7, c_t7,\n#                     h_t8, c_t8\n                   ):\n\n        \n        outputs = []\n\n        # encoder\n        for t in range(seq_len):\n            h_t, c_t = self.encoder_1_convlstm(input_tensor=x[:, t, :, :],\n                                               cur_state=[h_t, c_t])  # we could concat to provide skip conn here\n            h_t2, c_t2 = self.encoder_2_convlstm(input_tensor=h_t,\n                                                 cur_state=[h_t2, c_t2])  # we could concat to provide skip conn here\n#             h_t3, c_t3 = self.encoder_3_convlstm(input_tensor=h_t2,\n#                                                  cur_state=[h_t3, c_t3])  # we could concat to provide skip conn here\n#             h_t4, c_t4 = self.encoder_4_convlstm(input_tensor=h_t3,\n#                                                  cur_state=[h_t4, c_t4])  # we could concat to provide skip conn here\n            \n        del h_t, c_t\n\n        # encoder_vector\n        encoder_vector = h_t2\n        del h_t2, c_t2\n\n        # decoder\n        for t in range(future_step):\n            h_t5, c_t5 = self.decoder_1_convlstm(input_tensor=encoder_vector,\n                                                 cur_state=[h_t5, c_t5])  # we could concat to provide skip conn here\n\n            h_t6, c_t6 = self.decoder_2_convlstm(input_tensor=h_t5,\n                                                 cur_state=[h_t6, c_t6])  # we could concat to provide skip conn here\n#             h_t7, c_t7 = self.decoder_3_convlstm(input_tensor=h_t6,\n#                                                  cur_state=[h_t7, c_t7])  # we could concat to provide skip conn here\n\n#             h_t8, c_t8 = self.decoder_4_convlstm(input_tensor=h_t7,\n#                                                  cur_state=[h_t8, c_t8])  # we could concat to provide skip conn here\n            \n            encoder_vector = h_t6\n            outputs = outputs+[h_t6]  # predictions\n\n        del  h_t5, c_t5, h_t6, c_t6\n#         del h_t7, c_t7\n        \n        outputs = torch.stack(outputs, 1)\n        outputs = outputs.permute(0, 2, 1, 3, 4)\n        outputs = self.decoder_CNN(outputs)\n        if self.generator:\n            outputs = torch.nn.Tanh()(outputs)\n\n\n        return outputs\n    \n    def forward(self, x, future_seq=chan, hidden_state=None):\n\n        \"\"\"\n        Parameters\n        ----------\n        input_tensor:\n            5-D Tensor of shape (b, t, c, h, w)        #   batch, time, channel, height, width\n        \"\"\"\n        b, seq_len, c, h, w = x.size()\n        \n        # initialize hidden states\n        h_t, c_t = self.encoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n        h_t2, c_t2 = self.encoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n#         h_t3, c_t3 = self.encoder_3_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n#         h_t4, c_t4 = self.encoder_4_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n        h_t5, c_t5 = self.decoder_1_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n        h_t6, c_t6 = self.decoder_2_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n#         h_t7, c_t7 = self.decoder_3_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n#         h_t8, c_t8 = self.decoder_4_convlstm.init_hidden(batch_size=b, image_size=(h, w))\n\n        # autoencoder forward\n        outputs = self.autoencoder(x, seq_len, future_seq, \n                                   h_t, c_t,\n                                   h_t2, c_t2, \n#                                    h_t3, c_t3, \n#                                    h_t4, c_t4,\n                                   h_t5, c_t5, \n                                   h_t6, c_t6, \n#                                    h_t7, c_t7, \n#                                    h_t8, c_t8\n                                  )\n        del x, seq_len, future_seq, h_t, c_t, h_t2, c_t2\n        del h_t5, c_t5, h_t6, c_t6\n        \n        return outputs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}